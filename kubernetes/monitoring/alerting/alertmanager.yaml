# AlertManager Configuration
# Handles alert routing and notifications for homelab infrastructure

apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  name: alertmanager
  namespace: monitoring
  labels:
    app: alertmanager
spec:
  replicas: 1
  storage:
    volumeClaimTemplate:
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 1Gi
        storageClassName: local-path

  # Resource limits
  resources:
    requests:
      memory: 64Mi
      cpu: 50m
    limits:
      memory: 256Mi
      cpu: 200m

  # Security context
  securityContext:
    fsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
---
# AlertManager Service
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: monitoring
  labels:
    app: alertmanager
spec:
  type: ClusterIP
  ports:
  - name: web
    port: 9093
    targetPort: web
  selector:
    app.kubernetes.io/name: alertmanager
---
# AlertManager Configuration Secret
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-alertmanager
  namespace: monitoring
type: Opaque
stringData:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'localhost:587'
      smtp_from: 'alertmanager@vectorweight.local'

    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'web.hook'
      routes:
      - match:
          severity: critical
        receiver: critical-alerts
      - match:
          severity: warning
        receiver: warning-alerts
      - match:
          alertname: DeadMansSwitch
        receiver: deadmansswitch

    receivers:
    - name: 'web.hook'
      webhook_configs:
      - url: 'http://localhost:5001/'

    - name: 'critical-alerts'
      email_configs:
      - to: 'admin@vectorweight.local'
        subject: '[CRITICAL] {{ .GroupLabels.alertname }} - {{ .GroupLabels.cluster }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Labels:
          {{ range .Labels.SortedPairs }}  - {{ .Name }}: {{ .Value }}
          {{ end }}
          {{ end }}
      slack_configs:
      - api_url: 'YOUR_SLACK_WEBHOOK_URL'
        channel: '#alerts'
        title: 'Critical Alert - {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}{{ end }}'

    - name: 'warning-alerts'
      email_configs:
      - to: 'admin@vectorweight.local'
        subject: '[WARNING] {{ .GroupLabels.alertname }} - {{ .GroupLabels.cluster }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          {{ end }}

    - name: 'deadmansswitch'
      webhook_configs:
      - url: 'http://localhost:5001/deadmansswitch'

    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'dev', 'instance']
---
# AlertManager Ingress
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: alertmanager
  namespace: monitoring
  annotations:
    kubernetes.io/ingress.class: "nginx"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: alertmanager-auth
spec:
  tls:
  - hosts:
    - alertmanager.vectorweight.local
    secretName: alertmanager-tls
  rules:
  - host: alertmanager.vectorweight.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: alertmanager
            port:
              number: 9093
---
# Basic auth secret for AlertManager access
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-auth
  namespace: monitoring
type: Opaque
data:
  # admin:password (replace with actual bcrypt hash)
  auth: REPLACE_WITH_ACTUAL_BASIC_AUTH_HASH
---
# PrometheusRule for Infrastructure Alerts
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: infrastructure-alerts
  namespace: monitoring
  labels:
    team: homelab
spec:
  groups:
  - name: kubernetes-infrastructure
    rules:
    - alert: KubernetesNodeReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes node not ready"
        description: "Node {{ $labels.node }} has been unready for more than 10 minutes"

    - alert: KubernetesMemoryPressure
      expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes node memory pressure"
        description: "Node {{ $labels.node }} has memory pressure"

    - alert: KubernetesDiskPressure
      expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes node disk pressure"
        description: "Node {{ $labels.node }} has disk pressure"

    - alert: KubernetesOutOfDisk
      expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes node out of disk"
        description: "Node {{ $labels.node }} is out of disk space"

    - alert: KubernetesNodeNotReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes node not ready"
        description: "Node {{ $labels.node }} has been not ready for more than 10 minutes"

  - name: pod-alerts
    rules:
    - alert: KubernetesPodCrashLooping
      expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes pod crash looping"
        description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has restarted {{ $value }} times in the last hour"

    - alert: KubernetesPodNotReady
      expr: kube_pod_status_phase{phase=~"Failed|Pending|Unknown"} > 0
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes pod not ready"
        description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in {{ $labels.phase }} state for more than 15 minutes"

  - name: resource-alerts
    rules:
    - alert: HighCPUUsage
      expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage"
        description: "CPU usage is above 80% for more than 10 minutes on {{ $labels.instance }}"

    - alert: HighMemoryUsage
      expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 85
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "High memory usage"
        description: "Memory usage is above 85% for more than 10 minutes on {{ $labels.instance }}"

    - alert: DiskSpaceLow
      expr: (node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes * 100 > 85
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Disk space low"
        description: "Disk usage is above 85% on {{ $labels.instance }} {{ $labels.mountpoint }}"

  - name: application-alerts
    rules:
    - alert: ServiceDown
      expr: up == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Service is down"
        description: "Service {{ $labels.job }} has been down for more than 5 minutes"

    - alert: DeadMansSwitch
      expr: vector(1)
      labels:
        severity: none
      annotations:
        summary: "DeadMansSwitch"
        description: "This is a DeadMansSwitch meant to ensure that the entire alerting pipeline is functional"
